# MICrONS DataCleaner Docs

This webpage contains all the documentation for the [MICrONS DataCleaner](https://github.com/margheritapremi/MICrONS-datacleaner) pakcage. 

## Install üì•

You can install the package from PyPi via 

```python
pip install microns-datacleaner
```

## Quick Start ‚è©

For more detailed code, you can also check out the `basic_tutorial.ipynb` notebook inside `tutorial`.

### Download nucleus information 

The package is initiated by importing and generating an instance of the class, where you specify the `datadir` folder (where data will be downloaded), the `version` of the data you intend to download (see the [data releases](https://www.microns-explorer.org/manifests)).

```python
#Import the lib
import microns_datacleaner as mic

#Target version and download folder
cleaner = mic.MicronsDataCleaner(datadir = "data", version=1300, download_policy='minimum') 

#Download the data
cleaner.download_nucleus_data()
```

The `download_policy` stablishes which tables to be downloaded (see API below for details). The default mode, `minimum`, downloads only the tables which are required to build the unit table generated by the package.

The tables needs to be downloaded **only once**. They will be downloaded in `datadir/version/raw`. In this way, if you change the `version`, the tables will not be mixed, allowing for more replicable analyses.

Once the data is downloaded, the unit table can be constructed as 

```python
units, segments = cleaner.process_nucleus_data(functional_data=None)
```

The `segments` variable is a table with the resulting segmentation of the layers. The `units` includes all the information about the units. The package includes the information for all AIBS classified units. The position is already transformed into pial coordinates, with Y being the depth. Observe that the table includes also nonneuronal objects.

The raw dataset contains objects with duplicated `pt_root_id`, that can be overmerged objects, including fraction of an axon, spine head, or multisoma object. For this reason, the package works always with the `nucleus_id` when merging the tables. All objects that present more than `pt_root_id` are eliminated: it is not that _the duplicates_ are eliminates, but _all the objects_ that presented a duplicate. The `units` variable has unique `nucleus_id` and `pt_root_id`.

The flag `functional_data` allows for several options to integrate information from the functional dataset (see details in the API). In particular, it can add manually coregistrated `session`, `scan_idx` and `unit_id` in order to match the connectomics with the functional data. 

Finally, you might want to merge some information in the unit table that is not present but can be found in other annotation tables. The available tables can be easily obtained and downloaded with

```python
cleaner.get_table_list()

#Example download of the automatic coregistration
cleaner.download_tables(['coregistration_auto_phase3_fwd_apl_vess_combined_v2'])
```

The tables can be easily merged with the unit table using the convenience function `merge_columns`. Please refer to the `basic_tutorial` notebook for more examples.

```python
auto_coreg = cleaner.read_table("coregistration_auto_phase3_fwd_apl_vess_combined_v2")

#Add the columns from the auto_coreg table to our unit table
units = cleaner.merge_table(units, auto_coreg, columns=['session', 'scan_idx', 'unit_id', 'score'])
```

### Download synapses

The synapse table is too large to be downloaded entirely. For this reason, one needs to query the dataset with the desired subset of pre and post-synaptic neurons. This can be done only by specifying the `pt_root_id` of the desired units. One these have been selected, the synapses can be downloaded with    

```python
#All presynaptic inputs to a random sample of units 
postids = units['pt_root_id'].sample(n=50)
preids  = units['pt_root_id']
cleaner.download_synapse_data(preids, postids)
```
The `download_synapse_data` function allows to download the data in chunks of neurons of predefined size. In this way, if the server connection fails, part of the data was already written to file and download can re-start from where it was left. If a problem occurs, the function will retry automatically. Parameters such as the maximum number of trials before giving up, time between trials or download chunk size can be set. 

By default, `download_synapse_data` collapses all synapses between two units to a single effective connection. The synaptic size is the sum all of synapses found between the two units. By setting `drop_synapses_duplicates = False` one can retrieve all the individual synapses, as well as their position.  

Once all the chunks have been loaded, they need to be manually merged, indicating the name of the desired synapse table. 

```python
cleaner.merge_synapses(syn_table_name="example_merged_synapses")
```

### Filtering

The package includes a submodule `mic_datacleaner.filters` in order to easily query the constructed unit and connections tables. The function `fl.filter_neurons` can be used to subset neurons by several conditions at the same time:

```python
#All neurons that (1) are in area V1 (2) have proofread axons (3) are either in L2/3 or L4 
units_filter = fl.filter_neurons(units, layer=['L2/3', 'L4'], proofread='ax_clean', brain_area='V1')
```

More examples can be found on the `basic_tutorial` notebook. The synapses can be also filtered, 

```python
#All synapses with 
# - presynaptic neurons in V1 with axons clean
# - postsynaptic neurons in either RL, AL or LM
syn_filter = fl.filter_connections(units, synapses,  brain_area=['V1', ['RL', 'AL', 'LM']], proofread=['ax_clean', None]) 
```
or just by directly indicating the ids of the pre and post synaptic neurons that we want 

```python
syn_filter = fl.synapses_by_id(units, synapses, pre_ids=units_filter['pt_root_id'], post_ids=None) 
```

Please read the API documentation for filters to find our more.

### Remapping

When working with functional data and tunign curves it is useful to obtain the rates as a matrix to efficiently compute currents. This needs a remapping of the `pt_root_id`. Please check out the `basic_tutorial` notebook or the `remapper` subpackage docs to find out more.


## License and citation policy üìö

The code is shared under a license. When using our package, please consider citing it appropiately.

The data is downloaded from the [MICrONS Project](https://www.microns-explorer.org/) directly. Therefore, please follow also the [license and citation policy of the MICrONS dataset](https://www.microns-explorer.org/citation-policy). 

The repositories from which the data is extracted are 

1. [IARPA MICrONS Minnie Project](https://doi.org/10.60533/BOSS-2021-T0SY) 
2. [Microns Phase 3 NDA](https://github.com/cajal/microns_phase3_nda). 

To construct the tables the package downloads results derived from the following papers of the MICrONS collaboration: 

1. [Functional connectomics spanning multiple areas of mouse visual cortex](https://doi.org/10.1038/s41586-025-08790-w). The Microns Consortium. 2025 
2. [Foundation model of neural activity predicts response to new stimulus types](https://doi.org/10.1038/s41586-025-08829-y)
3. [Perisomatic ultrastructure efficiently classifies cells in mouse cortex](http://doi.org/10.1038/s41586-024-07765-7)
4. [NEURD offers automated proofreading and feature extraction for connectomics](https://doi.org/)
5. [CAVE: Connectome Annotation Versioning Engine](https://doi.org/10.1038/s41592-024-02426-z)

## Building the docs üìú 

To generate these docs locally in your computer, you need [pdoc](https://pdoc.dev/). Then, fromn the root directory of the repository, run

```
pdoc -t docs/template src/microns_datacleaner/ -o docs/html
```

The docs will be generated in the `docs/html` folder in HTML format, which can be checked with the browser. You can also get docs for individual just using `src/filename.py`.

